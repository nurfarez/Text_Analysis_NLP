# -*- coding: utf-8 -*-
"""TextAnalysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zP1a6yEcgWYuRgJhfby9XqoN2JsNxvgI
"""

import tensorflow as tf
print("TensorFlow version:", tf.__version__)

#Import Section
import os
import re
import json
import pickle
import datetime
import numpy as np
import pandas as pd

from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import LSTM,Dense,Dropout,Bidirectional,Embedding
from tensorflow.keras.preprocessing.text import Tokenizer,tokenizer_from_json
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import TensorBoard,EarlyStopping
from tensorflow.keras.models import Sequential,load_model
from tensorflow.keras.utils import plot_model
from tensorflow.keras import Input

#Path Section

URL = "https://raw.githubusercontent.com/susanli2016/PyCon-Canada-2019-NLP-Tutorial/master/bbc-text.csv"
LOGS_PATH = os.path.join(os.getcwd(),'logs',datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))

#%% EDA
# Step 1) Data loading
df = pd.read_csv(URL)

#Step 2) Data Inspection

#For text data inspection
df.info()
df.describe().T
df.head(7)

#can check up for any duplicates of text

df.duplicated().sum()
df.isna().sum()

# To recognize the unique targets
df['category'].unique()
#There are 5 categories:('tech', 'business', 'sport', 'entertainment', 'politics')

# To test out the text and category
print(df['text'][5])
print(df['category'][5])

# Step 3) Data cleaning
    # Process that need to be done before further analysis:
         #1. Need to remove duplicates

#remove duplicated data 
df= df.drop_duplicates()

# To remove numbers and converting to small letters (just to make sure no capitals)
text = df['text'].values # Features: X
category = df['category'].values # category: y

for index,txt in enumerate(text):
    text[index]=re.sub('<.*?>','',txt) 
    text[index]=re.sub('[^a-zA-Z]',' ',txt).lower().split()

#step 4) Features Selection
# no features need to be select as it is for text

#step 5)Data Preprocessing
# 1) Tokenization
#text need to used for tokenization

from tensorflow.keras.preprocessing.text import Tokenizer

vocab_size=10000
oov_token='<OOV>'

tokenizer=Tokenizer(num_words=vocab_size,oov_token=oov_token)
tokenizer.fit_on_texts(text)
word_index=tokenizer.word_index

print(dict(list(word_index.items())[0:10]))

text_int=tokenizer.texts_to_sequences(text)
text_int[1000]

#2) padding (to make sure all of them in equal length numbers)
# List comprehension
max_len=np.median([len(text_int[i])for i in range(len(text_int))])

from tensorflow.keras.preprocessing.sequence import pad_sequences

padded_text=pad_sequences(text_int,maxlen=int(max_len),padding='post',truncating='post')

# 3) One Hot Encoding for the Target
ohe = OneHotEncoder(sparse=False)
# This will become y
category = ohe.fit_transform(np.expand_dims(category,axis=-1))

# 4) Train test split

X_train,X_test,y_train,y_test = train_test_split(padded_text,category,
                                                 test_size=0.3,
                                                 random_state=123)

#%% Model development
    # Use LSTM layers,dropout,dense,input
    # achieve accuracy and F1 >70% 
    # Input shape --> np.shape(X_train)[1:] --> (340,1)

input_shape=np.shape(X_train)[1:]
out_dim=128

model = Sequential()
model.add(Input(shape=(input_shape)))
model.add(Embedding(vocab_size,out_dim))
model.add(Bidirectional(LSTM(out_dim,return_sequences=(True))))
model.add(Dropout(0.3))
model.add(LSTM(128))
model.add(Dropout(0.3))
model.add(Dense(5,'softmax'))
model.summary()


model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])

# To plot model architecture
plot_model(model)

#%%
#callbacks
tensorboard_callback=tf.keras.callbacks.TensorBoard(log_dir=LOGS_PATH,histogram_freq=1)
early_callback = EarlyStopping(monitor='loss',patience=5)
#%%
#Model Training
hist=model.fit(X_train,y_train,epochs=50,validation_data=(X_test,y_test),callbacks = [tensorboard_callback,early_callback])
#saving all the loss and hist model inside

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir logs

#%%

hist.history.keys()
print(hist.history.keys())

y_pred=np.argmax(model.predict(X_test),axis=1)
y_actual=np.argmax(y_test,axis=1)

print(classification_report(y_actual,y_pred))



MODEL_SAVE_PATH = os.path.join(os.getcwd(),'model','model.h5')
TOKENIZER_PATH = os.path.join(os.getcwd(),'model','tokenizer_text.json')
OHE_PATH = os.path.join(os.getcwd(),'model','ohe.pkl')

#%% Saving Model
# To save model
model.save(MODEL_SAVE_PATH)

# To save tokenizer
token_json = tokenizer.to_json()
with open(TOKENIZER_PATH,'w') as json_file:
    json.dump(token_json,json_file)

# To save OneHotEncoder
with open(OHE_PATH,'wb') as file:
    pickle.dump(ohe,file)